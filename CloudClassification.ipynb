{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset,random_split, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import netCDF4 as nc\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "npyPath = \"skogsstyrelsen-data\"\n",
    "ncPath = \"skogsstyrelsen-data/2A-netcdfs-cropped-from-nuria\"\n",
    "npyFiles =[\"skogs_json_test.npy\", \"skogs_json_train.npy\", \"skogs_json_val.npy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_loader():\n",
    "    \n",
    "    \n",
    "    # Initialize data_list outside the loop\n",
    "    data_list = []\n",
    "\n",
    "    # Load and print contents of each file\n",
    "    for file_name in npyFiles:\n",
    "        full_path = os.path.join(npyPath, file_name)  # Include folder path\n",
    "        if os.path.exists(full_path):  # Ensure file exists\n",
    "            data = np.load(full_path, allow_pickle=True)  # Load the .npy file\n",
    "            # Extract 'ValideringsobjektBildId' and 'MolnDis' from each JSON line\n",
    "            if isinstance(data, np.ndarray) and data.size > 0 and isinstance(data[0], dict):\n",
    "                for item in data:\n",
    "                    valideringsobjekt_bild_id = item.get('ValideringsobjektBildId', 'N/A')\n",
    "                    moln_dis = item.get('MolnDis', 'N/A')\n",
    "                    # Add to a list of lists\n",
    "                    data_list.append([valideringsobjekt_bild_id, moln_dis])\n",
    "            else:\n",
    "                print(f\"File {file_name} does not contain the expected JSON data.\")\n",
    "        else:\n",
    "            print(f\"File {file_name} does not exist.\")\n",
    "    print(f\"Loaded {len(data_list)} items.\")\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_bands(data_list):\n",
    "    bands = ['b02','b03', 'b04']\n",
    "    target_shape = (22, 22)  # Ensure uniform size\n",
    "\n",
    "    features = []\n",
    "    labels = []\n",
    "\n",
    "    for item in data_list:\n",
    "        file_id, label = item\n",
    "        file_path = os.path.join(ncPath, f'skgs_{file_id}.nc')\n",
    "\n",
    "        try:\n",
    "            with nc.Dataset(file_path, 'r') as dataset:\n",
    "                resampled_bands = []\n",
    "\n",
    "                for band in bands:\n",
    "                    if band in dataset.variables:\n",
    "                        band_data = dataset.variables[band][0, :, :]  # Extract first time slice\n",
    "                        band_data = np.nan_to_num(band_data, nan=0.0)  # Replace NaNs with 0\n",
    "\n",
    "                        # We use edge padding\n",
    "                        #\n",
    "                        if band_data.shape != target_shape:\n",
    "                            print(f\"Padding {file_id}: {band_data.shape} â†’ {target_shape}\")\n",
    "                            pad_width = get_padding(band_data.shape, target_shape)\n",
    "                            #\n",
    "                            band_data = np.pad(band_data, pad_width, mode='edge')\n",
    "                        \n",
    "\n",
    "                        resampled_bands.append(band_data)\n",
    "                    else:\n",
    "                        print(f\"Warning: {band} not found in {file_path}\")\n",
    "                        resampled_bands.append(np.zeros(target_shape))  # Default empty band\n",
    "\n",
    "                # Ensure we always have (4, 21, 21)\n",
    "                combined_bands = np.stack(resampled_bands, axis=0)  # (C, H, W)\n",
    "                print(f\"Final processed shape: {combined_bands.shape}\")  # Debugging\n",
    "\n",
    "                features.append(combined_bands)\n",
    "                labels.append(label)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Processed {len(features)} samples.\")\n",
    "\n",
    "    return np.array(features, dtype=np.float32), np.array(labels, dtype=np.int64)\n",
    "\n",
    "# Compute the padding required for each dimension\n",
    "def get_padding(band_shape, target_shape):\n",
    "    return [(0, max(0, target_shape[i] - band_shape[i])) for i in range(len(target_shape))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_std(train_dataset):\n",
    "    \n",
    "    mean_values = []\n",
    "    std_values = []\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=len(train_dataset), shuffle=False)\n",
    "    \n",
    "    for imgs, _ in train_loader:\n",
    "        num_channels = imgs.shape[1]\n",
    "        for channel in range(num_channels):\n",
    "            num_pixels_per_channel = imgs.shape[0] * imgs.shape[2] * imgs.shape[3] # number of images * image height * image width\n",
    "            mean = imgs[:,channel,:,:].sum() / num_pixels_per_channel\n",
    "            std = torch.sqrt((((imgs[:,channel,:,:] - mean) ** 2).sum()) / num_pixels_per_channel)\n",
    "            print(\"channel: \", channel, \"mean: \", mean.item(), \"std: \", std.item())\n",
    "            mean_values.append(mean.item())\n",
    "            std_values.append(std.item())\n",
    "\n",
    "    return mean_values, std_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SatelliteDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (numpy array or torch.Tensor): Multi-band satellite images (shape: N x C x H x W).\n",
    "            labels (numpy array or torch.Tensor): Binary labels (0 or 1).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)  # Convert to tensor\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)  # Convert to tensor\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.features[idx]  # Get image (shape: C x H x W)\n",
    "        label = self.labels[idx]    # Get corresponding label\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations\n",
    "\n",
    "        return image, label\n",
    "\n",
    "features, labels = combine_bands(npy_loader(),)\n",
    "\n",
    "print(features.shape, labels.shape)\n",
    "\n",
    "\n",
    "#dataset_transform = SatelliteDataset(features, labels, transform=transform)\n",
    "dataset = SatelliteDataset(features, labels)\n",
    "\n",
    "\n",
    "\n",
    "# Random split of the data\n",
    "validation_split = 0.25\n",
    "train_split = 0.6\n",
    "test_split = 0.15\n",
    "\n",
    "# Define the sizes for each split\n",
    "train_size = int(train_split * len(dataset))\n",
    "val_size = int(validation_split * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "generator = torch.Generator().manual_seed(31)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size], generator=generator)\n",
    "\n",
    "# Calculate the mean and std of the training set -> used to normalize train, val and testset\n",
    "mean, std = calculate_mean_std(train_dataset)\n",
    "\n",
    "# Get all cloud images from the training set\n",
    "# Get all cloud images (label 1) from the training set\n",
    "cloudy_indices = [i for i, (_, label) in enumerate(train_dataset) if label == 1]\n",
    "cloudy_dataset = torch.utils.data.Subset(train_dataset, cloudy_indices)\n",
    "\n",
    "# Clouddataset\n",
    "transformCloud = transforms.Compose([\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize using calculated mean and std\n",
    "])\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Apply random horizontal flipping\n",
    "    transforms.RandomRotation([0, 270]),  # Random rotation up to 40 degrees\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize using calculated mean and std\n",
    "])\n",
    "\n",
    "transform_normalize = transforms.Compose([\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalize values to [0, 1]\n",
    "])\n",
    "\n",
    "cloudy_dataset.dataset.transform = transformCloud\n",
    "train_dataset.dataset.transform = transform\n",
    "val_dataset.dataset.transform = transform_normalize\n",
    "test_dataset.dataset.transform = transform_normalize\n",
    "\n",
    "# Combine the datasets\n",
    "#train_dataset = ConcatDataset([train_dataset, cloudy_dataset])\n",
    "\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# Function to plot images from datasets\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of training samples:\", len(train_dataset))\n",
    "print(\"Number of validation samples:\", len(val_dataset))\n",
    "print(\"Number of testing samples:\", len(test_dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sizes = [train_size, val_size, test_size]\n",
    "labels = ['Training', 'Validation', 'Test']\n",
    "colors = ['#ff9999','#66b3ff','#99ff99']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes,labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "    shadow=True, startangle=140)\n",
    "plt.title('Dataset Split')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n",
    "\n",
    "# Pie chart of dataset splits\n",
    "total_size = len(dataset)\n",
    "train_size = len(train_dataset)\n",
    "val_size = len(val_dataset)\n",
    "test_size = len(test_dataset)\n",
    "\n",
    "train_percent = (train_size / total_size) * 100\n",
    "val_percent = (val_size / total_size) * 100\n",
    "test_percent = (test_size / total_size) * 100\n",
    "\n",
    "# Pie chart of dataset splits using percentages\n",
    "sizes = [train_percent, val_percent, test_percent]\n",
    "labels = ['Training', 'Validation', 'Test']\n",
    "colors = ['#ff9999','#66b3ff','#99ff99']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "    shadow=True, startangle=140)\n",
    "plt.title('Dataset Split (Percentage)')\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "\n",
    "def confusion_matrix(all_labels,all_predictions, class_labels=[0, 1]):\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = metrics.confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Set up the figure with a proper aspect ratio\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))  # Adjust figure size\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    cm_display.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "    # Adjust layout for a cleaner look\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "\n",
    "            nn.Flatten(),\n",
    "\n",
    "            nn.Linear(in_features=64*5*5, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        return self.layer(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "targets = []  # Collect all labels from train_loader\n",
    "for _, labels in train_loader:\n",
    "    targets.extend(labels.numpy())  # Assuming labels are tensors, convert them to numpy\n",
    "\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(targets), y=targets)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "#model = CloudDetectionCNN().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "train_loss = []\n",
    "val_loss_array = []\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss.append(running_loss/len(train_loader))\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "        # Save the best model\n",
    "        \n",
    "        # Save the best model based on validation accuracy\n",
    "        if epoch == 0 or (100 * val_correct / val_total) > best_accuracy:\n",
    "            best_accuracy = 100 * val_correct / val_total\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, 'cloudclass.pth')\n",
    "    val_loss_array.append(val_loss/len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {running_loss/len(train_loader):.2f} | Train Accuracy: {100 * train_correct / train_total:.2f}% | Validation Loss: {val_loss/len(val_loader):.2f} | Val Accuracy: {100 * val_correct / val_total:.2f}%\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss, skipping the first epoch\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(2, num_epochs + 1), train_loss[1:], marker='o', linestyle='-', color='b', label='Training Loss')\n",
    "plt.plot(range(2, num_epochs + 1), val_loss_array[1:], marker='o', linestyle='-', color='r', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    # Load the best model weights\n",
    "    model.load_state_dict(torch.load('cloudclass.pth'))\n",
    "    \n",
    "    for inputs, labels in test_loader:\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        print(correct)\n",
    "        print(total)\n",
    "        # Collect labels and predictions for f1 score\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Compute the f1 score\n",
    "f1 = f1_score(all_labels, all_preds)\n",
    "print(f\"Test Loss: {test_loss/len(test_loader)}, Accuracy: {100 * correct / total}%, F1 Score: {f1*100}%\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "confusion_matrix(all_labels,all_preds, class_labels=['No Clouds', 'Clouds'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN-Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ANNClassifier, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)  # Batch Normalization\n",
    "        self.relu1 = nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.relu2 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop1 = nn.Dropout(0.2) # Dropout for regularization\n",
    "\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.relu3 = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc4 = nn.Linear(64, num_classes) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu1(self.bn1(self.fc1(x)))\n",
    "        x = self.drop1(self.relu2(self.bn2(self.fc2(x))))\n",
    "        x = self.drop2(self.relu3(self.bn3(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    \n",
    "    train_loss_array = []\n",
    "    best_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1) \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "        train_acc = 100 * correct / total\n",
    "        training_loss = running_train_loss/len(train_loader)\n",
    "        train_loss_array.append(training_loss)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc, _, _ = test_model(val_loader, model)\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_wts, 'cloudclass.pth')\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {training_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "    print(f\"Best validation accuracy: {best_accuracy}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def test_model(loader, model = None):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Load the best model weights\n",
    "        if model == None:\n",
    "            model.load_state_dict(torch.load('cloudclass.pth'))\n",
    "\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            all_labels.extend(labels)\n",
    "            all_preds.extend(predicted)\n",
    "        \n",
    "        loss = running_loss/len(loader)\n",
    "        accuracy = 100 * correct / total\n",
    "            \n",
    "    return loss, accuracy, np.array(all_labels), np.array(all_preds)\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(all_labels = None, all_predictions = None, class_labels = [0, 1]):\n",
    "    \n",
    "    if all_labels is None or all_predictions is None:\n",
    "        print(\"Error: all_labels or all_predictions needs to be passed\")\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    cm = metrics.confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    # Set up the figure with a proper aspect ratio\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))  # Adjust figure size\n",
    "    cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
    "    cm_display.plot(cmap='Blues', values_format='d', ax=ax)\n",
    "\n",
    "    # Adjust layout for a cleaner look\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels aus dem Trainingsset extrahieren\n",
    "train_labels = [label for _, label in train_dataset]\n",
    "\n",
    "# Anzahl der Samples pro Klasse berechnen\n",
    "class_counts = np.bincount(train_labels)\n",
    "total_samples = len(train_labels)\n",
    "\n",
    "# Gewichte berechnen (inverse HÃ¤ufigkeit)\n",
    "class_weights = total_samples / (len(class_counts) * class_counts)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilisation and training\n",
    "input_size = 22 * 22 * 3\n",
    "num_classes = 2\n",
    "num_epochs = 200\n",
    "\n",
    "model = ANNClassifier(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "model = train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs)\n",
    "_, test_acc, all_labels, all_preds = test_model(test_loader, model)\n",
    "print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "print(\"F1 score: {:.2f}%\".format(f1_score(all_labels, all_preds) * 100))\n",
    "plot_confusion_matrix(all_labels, all_preds, class_labels=['No Clouds', 'Clouds'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
